{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bf9aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_loader as dl\n",
    "import autoencoder\n",
    "import trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20fff50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = autoencoder.Autoencoder(960, 1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "392326b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"input_data/GSE92742_Level5_isgold_lm_hf_modZ.csv\"\n",
    "train_loader, test_loader = dl.build_loaders(path, batch_size=64, split=0.9, in_format = 'z_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ef87aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Train loss: 0.10664689777609393\n",
      "Test loss: 0.00490136212682049\n",
      "Epoch:  1\n",
      "Train loss: 0.021934182903611227\n",
      "Test loss: 0.004055408005053416\n",
      "Epoch:  2\n",
      "Train loss: 0.0068637047738072075\n",
      "Test loss: 0.0037522377974282092\n",
      "Epoch:  3\n",
      "Train loss: 0.0070961479395097015\n",
      "Test loss: 0.0037754928075159205\n",
      "Epoch:  4\n",
      "Train loss: 0.027591189942481664\n",
      "Test loss: 0.00376583095244331\n",
      "Epoch:  5\n",
      "Train loss: 0.019146206509823245\n",
      "Test loss: 0.003687891594556152\n",
      "Epoch:  6\n",
      "Train loss: 0.015073390900237646\n",
      "Test loss: 0.0036463403512572504\n",
      "Epoch:  7\n",
      "Train loss: 0.020405545334929776\n",
      "Test loss: 0.0037274987144140987\n",
      "Epoch:  8\n",
      "Train loss: 0.02225469742702073\n",
      "Test loss: 0.004188448603432148\n",
      "Epoch:  9\n",
      "Train loss: 0.02203594993851394\n",
      "Test loss: 0.0036490439205892128\n",
      "Epoch:  10\n",
      "Train loss: 0.010472215722822047\n",
      "Test loss: 0.0037182827186412535\n",
      "Epoch:  11\n",
      "Train loss: 0.03051757965216503\n",
      "Test loss: 0.0037175778338892594\n",
      "Epoch:  12\n",
      "Train loss: 0.01385683361530865\n",
      "Test loss: 0.0036276645851440917\n",
      "Epoch:  13\n",
      "Train loss: 0.015896570149824575\n",
      "Test loss: 0.0037405634101432485\n",
      "Epoch:  14\n",
      "Train loss: 0.027826975592019968\n",
      "Test loss: 0.0036704560558940484\n",
      "Epoch:  15\n",
      "Train loss: 0.016448417255310967\n",
      "Test loss: 0.003657041745085237\n",
      "Epoch:  16\n",
      "Train loss: 0.015118733637150022\n",
      "Test loss: 0.003695006400728837\n",
      "Epoch:  17\n",
      "Train loss: 0.027399156463743475\n",
      "Test loss: 0.003644769762722282\n",
      "Epoch:  18\n",
      "Train loss: 0.01175455197224381\n",
      "Test loss: 0.003685964985066054\n",
      "Epoch:  19\n",
      "Train loss: 0.030389590307414855\n",
      "Test loss: 0.0040096984739041224\n",
      "Epoch:  20\n",
      "Train loss: 0.014345899456843076\n",
      "Test loss: 0.0036174795614221157\n",
      "Epoch:  21\n",
      "Train loss: 0.014773779495709127\n",
      "Test loss: 0.0036637087928003864\n",
      "Epoch:  22\n",
      "Train loss: 0.01849100685480684\n",
      "Test loss: 0.0037824697593847913\n",
      "Epoch:  23\n",
      "Train loss: 0.025304692758874335\n",
      "Test loss: 0.0036432108969196805\n",
      "Epoch:  24\n",
      "Train loss: 0.012816446755224685\n",
      "Test loss: 0.006125026957458283\n",
      "Epoch:  25\n",
      "Train loss: 0.029924623553418298\n",
      "Test loss: 0.006634800321494157\n",
      "Epoch:  26\n",
      "Train loss: 0.01365930217020589\n",
      "Test loss: 0.0036935274752581287\n",
      "Epoch:  27\n",
      "Train loss: 0.02040686207562488\n",
      "Test loss: 0.029431005547826107\n",
      "Epoch:  28\n",
      "Train loss: 0.021301653354386295\n",
      "Test loss: 0.0037115700433317284\n",
      "Epoch:  29\n",
      "Train loss: 0.018151916541174564\n",
      "Test loss: 0.003710231147451788\n",
      "Epoch:  30\n",
      "Train loss: 0.01845434456672422\n",
      "Test loss: 0.004071013433572191\n",
      "Epoch:  31\n",
      "Train loss: 0.023399063292924044\n",
      "Test loss: 0.003628031999358318\n",
      "Epoch:  32\n",
      "Train loss: 0.009962035764177559\n",
      "Test loss: 0.003698250615563339\n",
      "Epoch:  33\n",
      "Train loss: 0.030852643993857124\n",
      "Test loss: 0.0038685630176885006\n",
      "Epoch:  34\n",
      "Train loss: 0.013690708655853047\n",
      "Test loss: 0.003603183797122831\n",
      "Epoch:  35\n",
      "Train loss: 0.011794842583360762\n",
      "Test loss: 0.0037209982650442063\n",
      "Epoch:  36\n",
      "Train loss: 0.03062707368058663\n",
      "Test loss: 0.003911768513866979\n",
      "Epoch:  37\n",
      "Train loss: 0.015214736197009623\n",
      "Test loss: 0.0036108654876335117\n",
      "Epoch:  38\n",
      "Train loss: 0.00624369023685831\n",
      "Test loss: 0.0036076642290498013\n",
      "Epoch:  39\n",
      "Train loss: 0.020094381126755278\n",
      "Test loss: 0.0036879454840484083\n",
      "Epoch:  40\n",
      "Train loss: 0.023853387058398463\n",
      "Test loss: 0.0036006721806967168\n",
      "Epoch:  41\n",
      "Train loss: 0.015915984322338074\n",
      "Test loss: 0.0036522572758870245\n",
      "Epoch:  42\n",
      "Train loss: 0.02219927433264335\n",
      "Test loss: 0.0036723592546410284\n",
      "Epoch:  43\n",
      "Train loss: 0.021015902446823685\n",
      "Test loss: 0.0036153425918494025\n",
      "Epoch:  44\n",
      "Train loss: 0.012626646588752764\n",
      "Test loss: 0.003610149659550725\n",
      "Epoch:  45\n",
      "Train loss: 0.023795934987787145\n",
      "Test loss: 0.003727756430187987\n",
      "Epoch:  46\n",
      "Train loss: 0.019356316751734604\n",
      "Test loss: 0.0036101584123033616\n",
      "Epoch:  47\n",
      "Train loss: 0.019319112282206233\n",
      "Test loss: 0.0036339418094000248\n",
      "Epoch:  48\n",
      "Train loss: 0.01646468086377173\n",
      "Test loss: 0.0036302666203079062\n",
      "Epoch:  49\n",
      "Train loss: 0.02474584387788056\n",
      "Test loss: 0.003654474199375408\n",
      "Epoch:  50\n",
      "Train loss: 0.016108438276203323\n",
      "Test loss: 0.0036242219421248403\n",
      "Epoch:  51\n",
      "Train loss: 0.022587093763174717\n",
      "Test loss: 0.0036891592133344495\n",
      "Epoch:  52\n",
      "Train loss: 0.018163739771957864\n",
      "Test loss: 0.0038859795180555337\n",
      "Epoch:  53\n",
      "Train loss: 0.023635014547304968\n",
      "Test loss: 0.003653846149897983\n",
      "Epoch:  54\n",
      "Train loss: 0.01386147224025353\n",
      "Test loss: 0.0036594884415977022\n",
      "Epoch:  55\n",
      "Train loss: 0.02455842482807777\n",
      "Test loss: 0.0037810571084244764\n",
      "Epoch:  56\n",
      "Train loss: 0.017310438722664065\n",
      "Test loss: 0.0035973953196667455\n",
      "Epoch:  57\n",
      "Train loss: 0.016176291070949016\n",
      "Test loss: 0.003662215940463237\n",
      "Epoch:  58\n",
      "Train loss: 0.022037858670449675\n",
      "Test loss: 0.0037588883416376944\n",
      "Epoch:  59\n",
      "Train loss: 0.020841788807471628\n",
      "Test loss: 0.0036167575870123176\n",
      "Epoch:  60\n",
      "Train loss: 0.011795906609427241\n",
      "Test loss: 0.11849934706677738\n",
      "Epoch:  61\n",
      "Train loss: 0.031033184094250724\n",
      "Test loss: 0.0038112633031848657\n",
      "Epoch:  62\n",
      "Train loss: 0.012271236915019634\n",
      "Test loss: 0.0035910321552401936\n",
      "Epoch:  63\n",
      "Train loss: 0.013459883953997743\n",
      "Test loss: 0.003621150525748475\n",
      "Epoch:  64\n",
      "Train loss: 0.019792211915955718\n",
      "Test loss: 0.003751033877237485\n",
      "Epoch:  65\n",
      "Train loss: 0.023525459547940428\n",
      "Test loss: 0.003595912224923571\n",
      "Epoch:  66\n",
      "Train loss: 0.010830473701617384\n",
      "Test loss: 0.0036372486530667826\n",
      "Epoch:  67\n",
      "Train loss: 0.02830901481448647\n",
      "Test loss: 0.0038509373740635367\n",
      "Epoch:  68\n",
      "Train loss: 0.015541334951224907\n",
      "Test loss: 0.0037012237834736193\n",
      "Epoch:  69\n",
      "Train loss: 0.02204652329464708\n",
      "Test loss: 0.0037831052804891117\n",
      "Epoch:  70\n",
      "Train loss: 0.01800700063500479\n",
      "Test loss: 0.0035819987144289364\n",
      "Epoch:  71\n",
      "Train loss: 0.011345629972466851\n",
      "Test loss: 0.0037106553623054784\n",
      "Epoch:  72\n",
      "Train loss: 0.03278095103981068\n",
      "Test loss: 0.0037974085580700866\n",
      "Epoch:  73\n",
      "Train loss: 0.013716330321641795\n",
      "Test loss: 0.003638837247704848\n",
      "Epoch:  74\n",
      "Train loss: 0.017179197704429994\n",
      "Test loss: 0.0036211787532601093\n",
      "Epoch:  75\n",
      "Train loss: 0.02023876863880107\n",
      "Test loss: 0.0036093672171840048\n",
      "Epoch:  76\n",
      "Train loss: 0.016028293251372117\n",
      "Test loss: 0.003777857317808124\n",
      "Epoch:  77\n",
      "Train loss: 0.02677219178052189\n",
      "Test loss: 0.0037993335303587793\n",
      "Epoch:  78\n",
      "Train loss: 0.014708428391344324\n",
      "Test loss: 0.003601570691817846\n",
      "Epoch:  79\n",
      "Train loss: 0.01770100082892921\n",
      "Test loss: 0.003629771427116078\n",
      "Epoch:  80\n",
      "Train loss: 0.01898239149523111\n",
      "Test loss: 0.003596682324209529\n",
      "Epoch:  81\n",
      "Train loss: 0.019635157481302377\n",
      "Test loss: 0.0036875458975784226\n",
      "Epoch:  82\n",
      "Train loss: 0.02200560261683923\n",
      "Test loss: 0.003634432105212194\n",
      "Epoch:  83\n",
      "Train loss: 0.01847101983079227\n",
      "Test loss: 0.0036043392972718194\n",
      "Epoch:  84\n",
      "Train loss: 0.01176126462675538\n",
      "Test loss: 0.003604069710414634\n",
      "Epoch:  85\n",
      "Train loss: 0.02735034071054543\n",
      "Test loss: 0.003666699985281015\n",
      "Epoch:  86\n",
      "Train loss: 0.01535713929115252\n",
      "Test loss: 0.0035822319772062646\n",
      "Epoch:  87\n",
      "Train loss: 0.016667168981805303\n",
      "Test loss: 0.003784299165137813\n",
      "Epoch:  88\n",
      "Train loss: 0.02660728632442879\n",
      "Test loss: 0.0035744414194924835\n",
      "Epoch:  89\n",
      "Train loss: 0.01112974036539553\n",
      "Test loss: 0.0036109390794976143\n",
      "Epoch:  90\n",
      "Train loss: 0.01988606905150155\n",
      "Test loss: 0.0040017727866896195\n",
      "Epoch:  91\n",
      "Train loss: 0.023628514271856846\n",
      "Test loss: 0.003581899303608598\n",
      "Epoch:  92\n",
      "Train loss: 0.010011984273697688\n",
      "Test loss: 0.0035900387306434987\n",
      "Epoch:  93\n",
      "Train loss: 0.023253749657070045\n",
      "Test loss: 0.0037588094614732725\n",
      "Epoch:  94\n",
      "Train loss: 0.01937307985553489\n",
      "Test loss: 0.0035858692336294194\n",
      "Epoch:  95\n",
      "Train loss: 0.017867711932873714\n",
      "Test loss: 0.0037237602481698124\n",
      "Epoch:  96\n",
      "Train loss: 0.02198580020703817\n",
      "Test loss: 0.003570728096537865\n",
      "Epoch:  97\n",
      "Train loss: 0.015188317605559075\n",
      "Test loss: 0.003612568395705814\n",
      "Epoch:  98\n",
      "Train loss: 0.019946813517343252\n",
      "Test loss: 0.0036631575535155006\n",
      "Epoch:  99\n",
      "Train loss: 0.022386302389434322\n",
      "Test loss: 0.0037402595529316836\n",
      "Epoch:  100\n",
      "Train loss: 0.017660860806849952\n",
      "Test loss: 0.0036148006484294547\n",
      "Epoch:  101\n",
      "Train loss: 0.019653493271499504\n",
      "Test loss: 0.0035959706047916005\n",
      "Epoch:  102\n",
      "Train loss: 0.014129865814013995\n",
      "Test loss: 0.003661609988245699\n",
      "Epoch:  103\n",
      "Train loss: 0.027111594130016906\n",
      "Test loss: 0.003582246076060921\n",
      "Epoch:  104\n",
      "Train loss: 0.010684401731000673\n",
      "Test loss: 0.0035929732328105687\n",
      "Epoch:  105\n",
      "Train loss: 0.026982202837159262\n",
      "Test loss: 0.0036084397307827943\n",
      "Epoch:  106\n",
      "Train loss: 0.014320828035244202\n",
      "Test loss: 0.0037818883936374613\n",
      "Epoch:  107\n",
      "Train loss: 0.027207897324653854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0036174929013921536\n",
      "Epoch:  108\n",
      "Train loss: 0.012468825914936069\n",
      "Test loss: 0.0036458189252477425\n",
      "Epoch:  109\n",
      "Train loss: 0.025022488222109413\n",
      "Test loss: 0.0035917840839769597\n",
      "Epoch:  110\n",
      "Train loss: 0.01238927286303526\n",
      "Test loss: 0.0036342561417856277\n",
      "Epoch:  111\n",
      "Train loss: 0.02760890459028518\n",
      "Test loss: 0.003694602389366199\n",
      "Epoch:  112\n",
      "Train loss: 0.014229868125458225\n",
      "Test loss: 0.003570988573229466\n",
      "Epoch:  113\n",
      "Train loss: 0.017152110724912222\n",
      "Test loss: 0.0036985823735276348\n",
      "Epoch:  114\n",
      "Train loss: 0.023146611417335324\n",
      "Test loss: 0.0040318757949603925\n",
      "Epoch:  115\n",
      "Train loss: 0.018682154186577198\n",
      "Test loss: 0.0035750530243365683\n",
      "Epoch:  116\n",
      "Train loss: 0.008413762489629486\n",
      "Test loss: 0.003599201704180425\n",
      "Epoch:  117\n",
      "Train loss: 0.028689269888354952\n",
      "Test loss: 0.0036095891223949755\n",
      "Best loss:  0.003570728096537865\n"
     ]
    }
   ],
   "source": [
    "model_loss = trainer.train_AE(AE, train_loader, test_loader, method='Adam', learning_rate=1e-5, stop_threshold=1e-15, n_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f086338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'trainer' from 'C:\\\\Users\\\\Aaron Troy\\\\Documents\\\\Bioinformatics\\\\Drug repurposing autoencoder\\\\overParamAE\\\\trainer.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a19042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
